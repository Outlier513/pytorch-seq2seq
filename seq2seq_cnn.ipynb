{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import Multi30k\n",
    "from dataloader import *\n",
    "from utils import *\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "best_valid_loss = float(\"inf\")\n",
    "emb_dim = 256\n",
    "enc_hid_dim = dec_hid_dim = 512\n",
    "n_layers = 10\n",
    "dropout = 0.25\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = Multi30k(root=\"data\")\n",
    "train_dataset, val_dataset, test_dataset = (\n",
    "    to_map_style_dataset(train_dataset),\n",
    "    to_map_style_dataset(val_dataset),\n",
    "    to_map_style_dataset(test_dataset),\n",
    ")\n",
    "train_dataloader, val_dataloader, test_dataloader, etc = get_dataloader_and_etc(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    "    batch_size,\n",
    ")\n",
    "_, _, vocab_de, vocab_en = etc\n",
    "input_dim = len(vocab_de)\n",
    "output_dim = len(vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        emb_dim,\n",
    "        hid_dim,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        dropout,\n",
    "        max_length=100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    in_channels=hid_dim,\n",
    "                    out_channels=2 * hid_dim,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding=(kernel_size - 1) // 2,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "\n",
    "    def forward(self, src, pos, scale):\n",
    "        # src [bs, src_len]\n",
    "        batch_size, src_len = src.shape\n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        # tok_embedded [bs, src_len, emb_dim]\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        # pos_embedded [bs, src_len, emb_dim]\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        # embedded [bs, src_len, emb_dim]\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        # conv_input [bs, src_en, hid_dim]\n",
    "        conv_input = conv_input.permute(0, 2, 1)\n",
    "        # conv_input [bs, hid_dim, src_len]\n",
    "        for conv in self.convs:\n",
    "            conved = self.dropout(conv_input)\n",
    "            # conved [bs, hid_dim, src_len]\n",
    "            conved = conv(conved)\n",
    "            # conved [bs, hid_dim*2, src_len]\n",
    "            conved = F.glu(conved, dim=1)\n",
    "            # conved [bs, hid_dim, src_len]\n",
    "            conved = (conved + conv_input) * scale\n",
    "            # conved [bs, hid_dim, src_len]\n",
    "            conv_input = conved\n",
    "        conved = conved.permute(0, 2, 1)\n",
    "        # conved [bs, src_len, hid_dim]\n",
    "        conved = self.hid2emb(conved)\n",
    "        # conved [bs, src_len, emb_dim]\n",
    "        combined = (conved + embedded) * scale\n",
    "        # combined [bs, src_len, emb_dim]\n",
    "        conved = conved.permute(0, 2, 1)\n",
    "        # conved [bs, emb_dim, src_len]\n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim,\n",
    "        emb_dim,\n",
    "        hid_dim,\n",
    "        n_layers,\n",
    "        kernel_size,\n",
    "        dropout,\n",
    "        max_length=100,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [\n",
    "                nn.Conv1d(\n",
    "                    in_channels=hid_dim,\n",
    "                    out_channels=2 * hid_dim,\n",
    "                    kernel_size=kernel_size,\n",
    "                )\n",
    "                for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def calculate_attention(\n",
    "        self, embedded, conved, encoder_conved, encoder_combined, scale\n",
    "    ):\n",
    "        # conved [bs, trg_len, hid_dim]\n",
    "        conved_emb = self.attn_hid2emb(conved)\n",
    "        # conved_emb [bs, trg_len, emb_dim]\n",
    "        combined = (conved_emb + embedded) * scale\n",
    "        # combined [bs, trg_len, emb_dim]\n",
    "        # encoder_conved [bs, emb_dim, src_len]\n",
    "        energy = torch.matmul(combined, encoder_conved)\n",
    "        # energy [bs, trg_len, src_len]\n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        # attention [bs, trg_len, src_len]\n",
    "        # encoder_combined [bs, src_len, emb_dim]\n",
    "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
    "        # attended_encoding [bs, trg_len, emb_dim]\n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        # attended_encoding [bs, trg_len, hid_dim]\n",
    "        attended_combined = (conved + attended_encoding) * scale\n",
    "        # attended_encoding [bs, trg_len, hid_dim]\n",
    "        attended_combined = attended_combined.permute(0, 2, 1)\n",
    "        # attended_encoding [bs, hid_dim, trg_len]\n",
    "        return attention, attended_combined\n",
    "\n",
    "    def forward(self, trg, encoder_conved, encoder_combined, pos, padding, scale):\n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        # tok_embedded [bs, trg_len, emb_dim]\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        # tok_embedded [bs, trg_len, emb_dim]\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        # embedded [bs, trg_len, emb_dim]\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        # conv_input [bs, trg_len, hid_dim]\n",
    "        conv_input = conv_input.permute(0, 2, 1)\n",
    "        # conv_input [bs, hid_dim, trg_len]\n",
    "        for conv in self.convs:\n",
    "            conv_input = self.dropout(conv_input)\n",
    "            # padding [bs, hid_dim, kernel_size-1]\n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim=2)\n",
    "            # padded_conv_input [bs, hid_dim, trg_len+kernel_size-1]\n",
    "            conved = conv(padded_conv_input)\n",
    "            # conved [bs, hid_dim*2, trg_len]\n",
    "            conved = F.glu(conved, dim=1)\n",
    "            # conved [bs, hid_dim, trg_len]\n",
    "            conved = conved.permute(0, 2, 1)\n",
    "            # conved [bs, trg_len, hid_dim]\n",
    "            attention, conved = self.calculate_attention(\n",
    "                embedded, conved, encoder_conved, encoder_combined, scale\n",
    "            )\n",
    "            # conved [bs, hid_dim, trg_len)\n",
    "            conv_input = (conved + conv_input) * scale\n",
    "        conved = conv_input.permute(0, 2, 1)\n",
    "        conved = self.dropout(conved)\n",
    "        # conved [bs, trg_len, hid_dim]\n",
    "        conved = self.hid2emb(conved)\n",
    "        # conved [bs, trg_len, emb_dim]\n",
    "        output = self.fc_out(conved)\n",
    "        # output [bs, trg_len, output_dim]\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src = src.permute(1, 0)\n",
    "        trg = trg.permute(1, 0)\n",
    "        batch_size, src_len = src.shape\n",
    "        _, trg_len = trg.shape\n",
    "        src_pos = (\n",
    "            torch.arange(0, src_len)[None, :].repeat(batch_size, 1).to(self.device)\n",
    "        )\n",
    "        # src_pos [bs, src_len]\n",
    "        encoder_conved, encoder_combined = self.encoder(src, src_pos, self.scale)\n",
    "        trg_pos = (\n",
    "            torch.arange(0, trg_len)[None, :].repeat(batch_size, 1).to(self.device)\n",
    "        )\n",
    "        # src_pos [bs, src_len]\n",
    "        padding = torch.zeros(batch_size, dec_hid_dim, kernel_size - 1).to(self.device)\n",
    "        # padding [bs, hid_dim, kernel_size-1]\n",
    "        output, attention = self.decoder(\n",
    "            trg, encoder_conved, encoder_combined, trg_pos, padding, self.scale\n",
    "        )\n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 37,351,173 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(7853, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (emb2hid): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "    (convs): ModuleList(\n",
       "      (0-9): 10 x Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    )\n",
       "    (hid2emb): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(5893, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (emb2hid): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (attn_hid2emb): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (attn_emb2hid): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (convs): ModuleList(\n",
       "      (0-9): 10 x Conv1d(512, 1024, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "    (hid2emb): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (fc_out): Linear(in_features=256, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(input_dim, emb_dim, enc_hid_dim, n_layers, kernel_size, dropout)\n",
    "dec = Decoder(output_dim, emb_dim, dec_hid_dim, n_layers, kernel_size, dropout)\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "optimizer = Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "count_parameters(model)\n",
    "model.apply(init_weights3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 226/226 [00:13<00:00, 17.34it/s, train_loss=5.65]\n",
      "100%|██████████| 7/7 [00:00<00:00, 63.80it/s, eval_loss=4.47]\n",
      "Epoch: 2: 100%|██████████| 226/226 [00:11<00:00, 20.49it/s, train_loss=3.94]\n",
      "100%|██████████| 7/7 [00:00<00:00, 62.43it/s, eval_loss=3.7] \n",
      "Epoch: 3: 100%|██████████| 226/226 [00:10<00:00, 20.58it/s, train_loss=3.34]\n",
      "100%|██████████| 7/7 [00:00<00:00, 65.33it/s, eval_loss=3.15]\n",
      "Epoch: 4: 100%|██████████| 226/226 [00:10<00:00, 20.59it/s, train_loss=2.87]\n",
      "100%|██████████| 7/7 [00:00<00:00, 70.84it/s, eval_loss=2.69]\n",
      "Epoch: 5: 100%|██████████| 226/226 [00:10<00:00, 20.62it/s, train_loss=2.47]\n",
      "100%|██████████| 7/7 [00:00<00:00, 70.18it/s, eval_loss=2.35]\n",
      "Epoch: 6: 100%|██████████| 226/226 [00:11<00:00, 20.51it/s, train_loss=2.2] \n",
      "100%|██████████| 7/7 [00:00<00:00, 65.57it/s, eval_loss=2.19]\n",
      "Epoch: 7: 100%|██████████| 226/226 [00:10<00:00, 20.56it/s, train_loss=2.01]\n",
      "100%|██████████| 7/7 [00:00<00:00, 69.67it/s, eval_loss=2.11]\n",
      "Epoch: 8: 100%|██████████| 226/226 [00:10<00:00, 20.59it/s, train_loss=1.86]\n",
      "100%|██████████| 7/7 [00:00<00:00, 82.01it/s, eval_loss=1.9] \n",
      "Epoch: 9: 100%|██████████| 226/226 [00:10<00:00, 20.62it/s, train_loss=1.74]\n",
      "100%|██████████| 7/7 [00:00<00:00, 59.61it/s, eval_loss=2.01]\n",
      "Epoch: 10: 100%|██████████| 226/226 [00:10<00:00, 20.56it/s, train_loss=1.64]\n",
      "100%|██████████| 7/7 [00:00<00:00, 70.85it/s, eval_loss=1.97]\n"
     ]
    }
   ],
   "source": [
    "t_batch = math.ceil(len(train_dataset) // batch_size)\n",
    "v_batch = math.ceil(len(val_dataset) // batch_size)\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train(\n",
    "        epoch,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        t_batch,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        CLIP,\n",
    "        device,\n",
    "        mode=\"cnn\",\n",
    "    )\n",
    "    eval_loss = evaluate(model, val_dataloader, v_batch, criterion, device, mode='cnn')\n",
    "    if eval_loss < best_valid_loss:\n",
    "        best_valid_loss = eval_loss\n",
    "        torch.save(model.state_dict(), \"weight/tut5-model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 22.42it/s, eval_loss=1.95]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9496896437236242"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"weight/tut5-model.pt\"))\n",
    "t_batch = math.ceil(len(test_dataset) // batch_size)\n",
    "evaluate(model, test_dataloader, t_batch, criterion, device, mode='cnn')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
